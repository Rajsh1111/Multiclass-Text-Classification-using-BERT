# Multiclass-Text-Classification-using-BERT
**Business Objective for Multiclass Text Classification using BERT Project**
This project  covers the application of the BERT base model concerning text classification in detail. We will witness how this state-of-the-art Transformer model can achieve extremely high-performance metrics for a large corpus of data comprising more than 100k+ labeled training examples.
**Data Description for the BERT Text Classification Project**
For our case study, we have used the hugging face library datasets.
The BERT model is built on the AG News dataset.
•	AG News (AG’s News Corpus) is a sub-dataset of AG's corpus of news articles constructed by assembling titles and description fields of articles from the 4 largest classes
•	The four classes are: World, Sports, Business, Sci/Tech
•	The AG News contains 30,000 training and 1,900 test samples per class. 

**Aim of the Project Using BERT Model for Multiclass Classification**
The project aims at building, training, and fine-tuning the BERT model with respect to classification on the AG News dataset.

**Data Science Solution Approach for Project on Multiclass Classification using BERT is as follows:**
•	Checking the hardware acceleration settings.
•	Installing the required libraries
•	Checking for the available dataset from the hugging face library
•	Importing the required dataset
•	Loading the train and test data
•	Creating dataframe objects for train and test data.
•	Performing data pre-processing
•	Creating the BERT model.
•	Compile the BERT model.
•	Training the BERT model on some defined hyperparameters.
•	Evaluating the performance metrics
•	Saving the model.
 ***Tech Stack for Multiclass Text Classification Python Project***
•	Language - Python
•	Libraries – ktrain, transformers, datasets, numpy, pandas, tensorflow, timeit
•	Environment- Jupyter Notebook
